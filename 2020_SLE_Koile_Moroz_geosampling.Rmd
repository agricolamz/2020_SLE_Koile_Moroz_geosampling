---
title: "Detecting linguistic variation with geographic sampling"
author: "Ezequiel Koile, George Moroz"
institute: "Linguistic Convergence Laboratory, NRU HSE"
date: |
    | 26 August 2020
    |
    |    
    | Presentation is available here: \alert{tinyurl.com/y7kjsp67}
    | ![](images/00_qrcode.png)'
output: 
  beamer_presentation:
    df_print: kable
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: false
    includes:
      in_header: "config/presento.sty"
bibliography: bibliography.bib
biblio-style: "apalike"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# library(qrcode)
# png(filename="images/00_qrcode.png", width = 200, height = 200)
# qrcode_gen("https://github.com/agricolamz/2020_SLE_Koile_Moroz_geosampling/raw/master/2020_SLE_Koile_Moroz_geosampling.pdf")
# dev.off()
library(tidyverse)
theme_set(theme_bw())
df <- read_csv("data/fake_data.csv")
```

# Introduction

## Introduction

* Geolectal variation is often present in settings where one language is spoken across a vast geographic area [@labov1963social].
* It can be found in phonological, morphosyntactic, and lexical features. 
* Could be overlooked by linguists [@dorian10].

#The problem

## The problem

* Let us consider a geographical dialect continuum formed by a group of small villages [@chambers2004dialectology: 5--7]

* We are interested in spotting variation of a discrete parameter among the lects spoken on these villages

```{r, fig.height=5}
library(lingtypology)
map.feature(languages = circassian$language,
            features = circassian$language,
            latitude = circassian$latitude,
            longitude = circassian$longitude, 
            minimap = TRUE,
            minimap.position = "topright",
            legend.position = "bottomleft",
            tile = "Esri.WorldTopoMap")
```

## The problem

* We will very unlikely be able to conduct fieldwork in each single village. Therefore, we need to choose a *sample* of locations.

* *Research Question*: How to choose the sample of villages to survey? 
    1) How many villages is enough for detecting all variation present? (number of categories)
    2) How many villages is enough for estimating the variation? (proportion of each category)
    3) Given an amount of sampled villages, how to decide which ones are representative of our population?

```{r, fig.height=5}
map.feature(languages = circassian$language,
            features = circassian$dialect,
            latitude = circassian$latitude,
            longitude = circassian$longitude, 
            minimap = TRUE,
            minimap.position = "topright",
            legend.position = "bottomleft",
            tile = "Esri.WorldTopoMap")
```


# Our approach

## Our approach

* We want to find the distribution of variation for one feature, and we try different ways of choosing the sampled villages for finding it

* As we assume we don't have any data beyond the geographic location of each village, we use these locations for building our sample

* We generate clusters with different algorithms (k-means, hierarchical clustering) and pick our sampled locations based on them (package stats, [@rteam]).

* We compare our results against random sampling in two different scenarios, both for simulated and for real Circassian data: 
    * Multiple categorical data (detect variation)
    * Binary categorical data (estimate variation)

    

## Information entropy

In order to measure the diversity of the questions we used the easiest measure --- information entropy, introduced in [@shannon48]:

$$H(X) = - \sum_{i = 1}^n{P(x_i)\times\log_2P(x_i)}$$

The range of the information entropy is $H(X) \in [0, +\infty]$: 

```{r}
tibble(a = c("A", "A", "A", "A", "B"),
       b = c("A", "A", "A", "B", "B"),
       c = c("A", "A", "B", "B", "B"),
       e = c("A", "A", "B", "B", "C"),
       f = c("A", "B", "C", "A", "B"),
       g = c("A", "A", "A", "A", "A"), 
       h = c("A", "B", "C", "D", "E")) %>% 
  pivot_longer(names_to = "id", values_to = "value", a:h) %>% 
  group_by(id) %>% 
  mutate(data = str_c(value, collapse = "-")) %>% 
  count(data, value) %>% 
  mutate(ratio = n/sum(n)) %>% 
  group_by(data) %>% 
  summarise(entropy = round(-sum(ratio*log2(ratio)), 2)) %>% 
  arrange(entropy) %>% 
  knitr::kable()
```

# Simulated data

## Simulated data

* total number of locations (*N*): `r str_c(unique(df$N), "", collapse = ", ")`
* type of spatial relations:
    * random
    * two more or less separable regions
    * central and periphery
* proportion of variation in the explored variable (*p*): `r str_c(unique(df$p), "", collapse = ", ")`
* amount of clusters (*k*): $2, \dots N/2$
* percantage of observations taken from each cluster (*r*): $0.1, 0.2, \dots$

From  those values we could derive a number of sampled locations (n):

$$n = N\times r$$

## Example of different number of locations (*N*)

```{r}
df %>% 
  filter(id %in% 1:4) %>% 
  ggplot(aes(x, y))+
  geom_point()+
  facet_wrap(~N)+
  labs(x = "", y = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```


## Example of different types of spatial relations

```{r}
df %>% 
  filter(N == 120, p == 0.4, r == 0.1) %>% 
  mutate(spatial_relations = factor(spatial_relations, levels = c("random", "two_regions", "central_periphery"), labels = c("random", "two regions", "central-periphery"))) %>% 
  ggplot(aes(x, y, color = value, shape = value))+
  stat_ellipse(alpha = 0.2)+
  geom_point()+
  facet_wrap(~spatial_relations, scale = "free", nrow = 2)+
  labs(x = "", y = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

## Example of different proportions of variation in the explored variable (*p*)

```{r}
df %>% 
  filter(N == 90, r == 0.1, spatial_relations == "random") %>% 
  group_by(id) %>% 
  summarise(a = sum(value == "a"),
            b = sum(value == "b"),
            x = max(x)-0.3,
            y = max(y)-0.3,
            p = p,
            label = str_c("a = ", a, "\nb = ", b)) %>% 
  distinct() ->
  labels 

df %>% 
  filter(N == 90, r == 0.1, spatial_relations == "random") %>% 
  ggplot(aes(x, y))+
  stat_ellipse(alpha = 0.2, aes(color = value))+
  geom_point(aes(color = value, shape = value))+
  facet_wrap(~p, scale = "free", nrow = 2)+
  geom_text(data = labels, aes(label = label, x = x, y = y))+
  labs(x = "", y = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

# Circassian data example

# Conclusion

# References {.allowframebreaks}
