---
title: "Detecting linguistic variation with geographic sampling"
author: "Ezequiel Koile, George Moroz"
institute: "Linguistic Convergence Laboratory, NRU HSE"
date: |
    | 26 August 2020
    |
    |    
    | Presentation is available here: \alert{tinyurl.com/y7kjsp67}
    | ![](images/00_qrcode.png)'
output: 
  beamer_presentation:
    df_print: kable
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: false
    includes:
      in_header: "config/presento.sty"
bibliography: bibliography.bib
biblio-style: "apalike"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(digits = 3)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# library(qrcode)
# png(filename="images/00_qrcode.png", width = 200, height = 200)
# qrcode_gen("https://github.com/agricolamz/2020_SLE_Koile_Moroz_geosampling/raw/master/2020_SLE_Koile_Moroz_geosampling.pdf")
# dev.off()
library(tidyverse)
library(broom)
library(ggeffects)
theme_set(theme_bw()+
            theme(text = element_text(size = 15),
                  legend.position="bottom"))
generated_data <- read_csv("data/generated_data.csv")
generated_data %>% 
  mutate(type = ifelse(type == "random", "uniform", type),
         type = factor(type, levels = c("uniform", "equadistant", "central-periphery"))) ->
  generated_data
all_results <- read_csv("data/all_results.csv")
all_results %>% 
  mutate(type = ifelse(type == "random", "uniform", type),
         type = factor(type, levels = c("uniform", "equadistant", "central-periphery")),
         cluster_type = factor(cluster_type, levels = c("random", "k-means", "hierarchical clustering")),
         ratio_binary = ifelse(ratio == 1, 1, 0)) ->
  all_results
```

# Introduction

## Introduction

* Geolectal variation is often present in settings where one language is spoken across a vast geographic area [@labov1963social].
* It can be found in phonological, morphosyntactic, and lexical features. 
* It could be overlooked by linguists [@dorian10].

# The problem

## The problem

* Let us consider a geographical dialect continuum formed by a group of small villages [@chambers2004dialectology: 5--7]

* We are interested in spotting variation of a discrete parameter among the lects spoken on these villages

```{r, fig.height=5}
library(lingtypology)
map.feature(languages = circassian$language,
            features = circassian$language,
            latitude = circassian$latitude,
            longitude = circassian$longitude, 
            minimap = TRUE,
            minimap.position = "topright",
            legend.position = "bottomleft",
            tile = "Esri.WorldTopoMap")
```

## The problem

* We will very unlikely be able to conduct fieldwork in each single village. Therefore, we need to choose a *sample* of locations.

* *Research Question*: How to choose the sample of villages to survey? 
    1) How many villages is enough for detecting all variation present? (number of categories)
    2) Given an amount of sampled villages, how to decide which ones are representative of our population?

```{r, fig.height=5}
map.feature(languages = circassian$language,
            features = circassian$dialect,
            latitude = circassian$latitude,
            longitude = circassian$longitude, 
            minimap = TRUE,
            minimap.position = "topright",
            legend.position = "bottomleft",
            tile = "Esri.WorldTopoMap")
```


# Our approach

## Our approach

* We want to find the amount of variation present for one feature, and we try different ways of choosing the sampled villages for finding it

* As we assume we do not have any data beyond the geographic location of each village, we use these locations for building our sample

* We generate clusters with different algorithms (k-means, hierarchical clustering) and pick our sampled locations based on them (package stats, [@rteam]).

* We compare our results against random geographic sampling for multiple categorical data, in two different scenarios:
    * Simulated data
    * Dialects of Circassian languages

# Simulated data

## Simulated data

### Data

* total number of locations (*N*): $30, 50, 70$
* number of categories (*n*): $3, 4, 5$
* type of spatial configuration:
    * uniform: variation is uniformly distributed across space
    * equadistant: *n* groups with unique values, partially overlaping 
    * central-periphery: one main group in the center, the rest around it
* count configuration (*c*): how the *n* categories are distributed across the *N* locations (e.g., for *N*=30, *n*=3, the count configuration could be *c*=10-10-10, *c*=20-8-2, etc.)

### Sampling

* clustering method: hierarchical clustering, *k*-means, random sampling
* proportion of villages sampled: $p = 0.05, 0.10, \ldots, 0.90$

From  those values we could derive the number of sampled locations, or number of clusters ($k$): $k = p\times N$

## Example of different number of locations (*N*)

```{r}
set.seed(42)
generated_data %>% 
  group_by(n_villages) %>% 
  sample_n(1) %>% 
  ungroup() %>% 
  select(dataset_id) %>% 
  unlist() %>% 
  unname() ->
  select_n_villages

generated_data %>% 
  filter(dataset_id %in% select_n_villages) %>% 
  ggplot(aes(x, y))+
  geom_point()+
  facet_wrap(~n_villages, nrow = 2)+
  labs(x = "", y = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

## Example of different number of categories (*n*)

```{r}
set.seed(42)
generated_data %>% 
  group_by(n_category) %>% 
  sample_n(1) %>% 
  ungroup() %>% 
  select(dataset_id) %>% 
  unlist() %>% 
  unname() ->
  select_n_category

generated_data %>% 
  filter(dataset_id %in% select_n_category) %>% 
  ggplot(aes(x, y, color = value, shape = value))+
  geom_point(size = 2)+
  facet_wrap(~n_category, nrow = 2)+
  labs(x = "", y = "", color = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())+
  scale_shape_manual("", values=c(15,16,17,18, 8))
```


## Example of different count configurations (*c*)

```{r}
set.seed(42)
generated_data %>% 
  filter(n_category == 4, n_villages == 30) %>% 
  group_by(vars) %>% 
  sample_n(1) %>% 
  ungroup() %>%
  sample_n(4) %>% 
  select(dataset_id) %>% 
  unlist() %>% 
  unname() ->
  select_vars

generated_data %>% 
  filter(dataset_id %in% select_vars) %>% 
  ggplot(aes(x, y, color = value, shape = value))+
  geom_point(size = 2)+
  facet_wrap(~vars, nrow = 2)+
  labs(x = "", y = "", color = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())+
  scale_shape_manual("", values=c(15,16,17,18))
```

## Example of different types of spatial configurations

```{r}
set.seed(42)
generated_data %>% 
  filter(n_category == 4, n_villages == 70, vars == "22-18-17-13") %>% 
  group_by(type) %>% 
  sample_n(1) %>% 
  ungroup() %>%
  select(dataset_id) %>% 
  unlist() %>% 
  unname() ->
  select_type

generated_data %>% 
  filter(dataset_id %in% select_type) %>% 
  ggplot(aes(x, y, color = value, shape = value))+
  geom_point(size = 2)+
  stat_ellipse()+
  facet_wrap(~type, nrow = 2)+
  labs(x = "", y = "", color = "")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())+
  scale_shape_manual("", values=c(15,16,17,18))

# df %>% 
#   filter(N == 120, p == 0.4, r == 0.1) %>% 
#   mutate(spatial_relations = factor(spatial_relations, levels = c("random", "two_regions", "central_periphery"), labels = c("random", "two regions", "central-periphery"))) %>% 
#   ggplot(aes(x, y, color = value, shape = value, shape = value))+
#   stat_ellipse(alpha = 0.2)+
#   geom_point()+
#   facet_wrap(~spatial_relations, scale = "free", nrow = 2)+
#   labs(x = "", y = "")+
#   theme(axis.text = element_blank(),
#         axis.ticks = element_blank())
```


# Results and modelling

## Results by number of villages

```{r}
all_results %>%   
  ggplot(aes(proportion_of_village, ratio, color = cluster_type))+
  geom_jitter(alpha = 0.02, size = 0.7)+
  geom_smooth(se = FALSE)+
  facet_wrap(~n_villages, nrow = 2)+
  labs(y = "proportion of variation discovered",
       x = "proportion of villages sampled",
       color = "")
```

## Results by number of categories

```{r}
all_results %>%   
  ggplot(aes(proportion_of_village, ratio, color = cluster_type))+
  geom_jitter(alpha = 0.02, size = 0.7)+
  geom_smooth(se = FALSE)+
  facet_wrap(~n_category, nrow = 2)+
  labs(y = "proportion of variation discovered",
       x = "proportion of villages sampled",
       color = "")
```


## Results by type of spatial relation

```{r}
all_results %>%   
  ggplot(aes(proportion_of_village, ratio, color = cluster_type))+
  geom_jitter(alpha = 0.02, size = 0.7)+
  geom_smooth(se = FALSE)+
  facet_wrap(~type, nrow = 2)+
  labs(y = "proportion of variation discovered",
       x = "proportion of villages sampled",
       color = "")
```

## Modelling the variation

* From the previous slides, we can see that 
  * *k*-means and hierarchical clustering are signifficantly better than random sampling in non-uniform spatial relations;
  * *k*-means and hierarchical clustering are as good as random sampling with uniform spatial relations.

* We run a logistic regression in order to prove those observations by quantifying the relation between:
  * One **binary variable** (All variation discovered vs. Not all variation discovered)
  * And three parameters:  
      * Proportion of villages sampled $p$ (numeric: $0.05, \ldots, 0.90$)
      * Type of clustering (hirarchical, *k*-means, random)
      * Type of geographic distribution (central-periphery, equidistant, uniform)
  
outcome ~ (spatial configuration + cluster type) * proportion_villages
             
## Regression results

\small
```{r}
fit_1 <- glm(ratio_binary ~ (type+cluster_type)*proportion_of_village,
             family = "binomial",
             data=all_results)
# tidy(fit_1) %>% 
#   write_csv("data/fit_1.csv")
read_csv("data/fit_1.csv") %>% 
  knitr::kable()
```

\normalsize

## Regression results
```{r}
ggeffect(fit_1, terms = c("proportion_of_village", "cluster_type", "type")) %>% 
  plot()+
  theme(text = element_text(size = 15),
                  legend.position="bottom")+
  labs(y = "proportion of variation discovered",
       x = "proportion of villages sampled",
       color = "")
```
  
# Circassian data example

# Entropy

## Information entropy

In order to measure how the count configuration $c$ affects our sampling method, we used the information entropy, introduced in [@shannon48]:

$$H(X) = - \sum_{i = 1}^n{P(x_i)\times\log_2P(x_i)}$$
\pause

The range of the information entropy is $H(X) \in [0, +\infty]$: 

```{r}
tibble(a = c("A", "A", "A", "A", "B"),
       b = c("A", "A", "A", "B", "B"),
       c = c("A", "A", "B", "B", "B"),
       e = c("A", "A", "B", "B", "C"),
       f = c("A", "B", "C", "A", "B"),
       g = c("A", "A", "A", "A", "A"), 
       h = c("A", "B", "C", "D", "E")) %>% 
  pivot_longer(names_to = "id", values_to = "value", a:h) %>% 
  group_by(id) %>% 
  mutate(data = str_c(value, collapse = "-")) %>% 
  count(data, value) %>% 
  mutate(ratio = n/sum(n)) %>% 
  group_by(data) %>% 
  summarise(entropy = round(-sum(ratio*log2(ratio)), 2)) %>% 
  arrange(entropy) %>% 
  knitr::kable()
```

## Information entropy: simulated data

```{r}
all_results %>%   
  ggplot(aes(H, ratio, linetype = factor(n_category), color = cluster_type))+
  #geom_jitter(alpha = 0.2, size = 0.7)+
  geom_smooth(se = FALSE)+
  scale_linetype_manual("", values=c(1,2,4))+
  labs(x = "entropy", y = "proportion of variation discovered",
       color = "", linetype = "")
```

## Information entropy: simulated data

* We can see that our sampling method performs considerably better than random for lower values of entropy, as expected.

# Conclusions

* We introduced a geographical sampling method for detecting variation in languages, based on different clustering methods
* We tested our method against random sampling in data simulated with different geographical distribution and size
* We applied our method to real data from Circassian languages
* We have found that our method outperforms random sampling in the cases where an underlying geographical structure is present, and performs equally well as it when variation is uniformly distributed across space
* We found that our method has optimal results when entropy is lower


## Conclusions

# References {.allowframebreaks}
